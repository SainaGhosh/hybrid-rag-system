{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d04a0f0a",
   "metadata": {},
   "source": [
    "# Hybrid RAG System - Run Streamlit App in Google Colab\n",
    "This notebook demonstrates how to run your Hybrid RAG System Streamlit app in Google Colab with public URL access using Ngrok tunneling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2a42c7",
   "metadata": {},
   "source": [
    "## Section 1: Install Streamlit and Dependencies\n",
    "Install all required packages for the Hybrid RAG System."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb26e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q streamlit pyngrok requests beautifulsoup4 numpy faiss-cpu sentence-transformers rank-bm25 transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d5a0c5",
   "metadata": {},
   "source": [
    "## Section 2: Set Up Ngrok Authentication\n",
    "Get your Ngrok token from https://dashboard.ngrok.com/auth/your-authtoken and add it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095ca308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Ngrok authentication token\n",
    "from pyngrok import ngrok\n",
    "\n",
    "# Replace 'your_token_here' with your actual Ngrok token\n",
    "ngrok.set_auth_token(\"your_token_here\")\n",
    "\n",
    "# Optional: Configure Ngrok\n",
    "ngrok_tunnel = ngrok.connect(8501)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66945790",
   "metadata": {},
   "source": [
    "## Section 3: Create Modified hybrid_rag_system.py for Colab\n",
    "Streamlit requires modifications to run in Colab. Here's the adapted version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456618da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hybrid_rag_colab.py\n",
    "import json\n",
    "import random\n",
    "import requests\n",
    "import time\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import pipeline\n",
    "import streamlit as st\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Step 1: Load Wikipedia URLs (200 Fixed + 300 randomly scraped)\n",
    "# ---------------------------------------------------------------\n",
    "def load_urls(use_fixed=True):\n",
    "    \"\"\"Load Wikipedia URLs for RAG system\"\"\"\n",
    "    urls = []\n",
    "    \n",
    "    # Use sample URLs for demo\n",
    "    sample_urls = [\n",
    "        \"https://en.wikipedia.org/wiki/Machine_learning\",\n",
    "        \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
    "        \"https://en.wikipedia.org/wiki/Deep_learning\",\n",
    "        \"https://en.wikipedia.org/wiki/Natural_language_processing\",\n",
    "    ]\n",
    "    return sample_urls\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Step 2: Extract and Chunk Text\n",
    "# ---------------------------------------------------------------\n",
    "def chunk_text(text, chunk_size=300, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = \" \".join(words[i:i+chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Step 3: Dense Vector Index (FAISS)\n",
    "# ---------------------------------------------------------------\n",
    "def build_dense_index(chunks, model_name=\"all-MiniLM-L6-v2\"):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(chunks, convert_to_numpy=True)\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(embeddings)\n",
    "    return index, embeddings, model\n",
    "\n",
    "def dense_retrieve(query, index, model, chunks, top_k=5):\n",
    "    q_emb = model.encode([query], convert_to_numpy=True)\n",
    "    scores, ids = index.search(q_emb, min(top_k, len(chunks)))\n",
    "    return [(chunks[i], float(scores[0][j])) for j,i in enumerate(ids[0])]\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Step 4: Sparse Retrieval (BM25)\n",
    "# ---------------------------------------------------------------\n",
    "def build_sparse_index(chunks):\n",
    "    tokenized = [chunk.split() for chunk in chunks]\n",
    "    bm25 = BM25Okapi(tokenized)\n",
    "    return bm25, tokenized\n",
    "\n",
    "def sparse_retrieve(query, bm25, chunks, top_k=5):\n",
    "    scores = bm25.get_scores(query.split())\n",
    "    ranked = np.argsort(scores)[::-1][:min(top_k, len(chunks))]\n",
    "    return [(chunks[i], float(scores[i])) for i in ranked]\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Step 5: Reciprocal Rank Fusion\n",
    "# ---------------------------------------------------------------\n",
    "def reciprocal_rank_fusion(dense_results, sparse_results, k=60, top_n=5):\n",
    "    scores = {}\n",
    "    for rank, (chunk, _) in enumerate(dense_results):\n",
    "        scores[chunk] = scores.get(chunk, 0) + 1/(k+rank+1)\n",
    "    for rank, (chunk, _) in enumerate(sparse_results):\n",
    "        scores[chunk] = scores.get(chunk, 0) + 1/(k+rank+1)\n",
    "    fused = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    return fused\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Step 6: Streamlit UI (Colab Compatible)\n",
    "# ---------------------------------------------------------------\n",
    "def run_ui():\n",
    "    st.set_page_config(page_title=\"Hybrid RAG System\", layout=\"wide\")\n",
    "    st.title(\"üîç Hybrid RAG System (Dense + BM25 + RRF)\")\n",
    "    \n",
    "    # Demo corpus\n",
    "    demo_corpus = [\n",
    "        \"Machine learning is a subset of artificial intelligence that focuses on learning from data.\",\n",
    "        \"Deep learning uses neural networks with multiple layers to learn representations.\",\n",
    "        \"Natural language processing enables computers to understand and generate human language.\",\n",
    "        \"Information retrieval systems help find relevant documents from large collections.\",\n",
    "    ]\n",
    "    \n",
    "    chunks = []\n",
    "    for doc in demo_corpus:\n",
    "        chunks.extend(chunk_text(doc))\n",
    "    \n",
    "    if not chunks:\n",
    "        st.warning(\"No documents loaded. Using sample data.\")\n",
    "        chunks = demo_corpus\n",
    "    \n",
    "    # Build indices\n",
    "    dense_index, embeddings, dense_model = build_dense_index(chunks)\n",
    "    bm25, tokenized = build_sparse_index(chunks)\n",
    "    \n",
    "    # Query input\n",
    "    query = st.text_input(\"üìù Enter your question:\", placeholder=\"Ask me something...\")\n",
    "    \n",
    "    if query:\n",
    "        with st.spinner(\"Retrieving and processing...\"):\n",
    "            start = time.time()\n",
    "            \n",
    "            # Retrieve results\n",
    "            dense_results = dense_retrieve(query, dense_index, dense_model, chunks, top_k=3)\n",
    "            sparse_results = sparse_retrieve(query, bm25, chunks, top_k=3)\n",
    "            rrf_results = reciprocal_rank_fusion(dense_results, sparse_results, top_n=3)\n",
    "            \n",
    "            end = time.time()\n",
    "        \n",
    "        # Display results\n",
    "        st.subheader(\"üìä Top Retrieved Chunks\")\n",
    "        for i, (chunk, score) in enumerate(rrf_results, 1):\n",
    "            with st.expander(f\"Result {i} (Score: {score:.4f})\"):\n",
    "                st.write(chunk)\n",
    "        \n",
    "        st.success(f\"‚úÖ Response Time: {end-start:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba59233d",
   "metadata": {},
   "source": [
    "## Section 4: Configure Colab for Streamlit Execution\n",
    "Setup environment variables and necessary configurations for Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f63237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import threading\n",
    "\n",
    "# Configure Streamlit for Colab\n",
    "os.environ['STREAMLIT_SERVER_HEADLESS'] = 'true'\n",
    "os.environ['STREAMLIT_SERVER_PORT'] = '8501'\n",
    "os.environ['STREAMLIT_SERVER_ENABLE_XSRF_PROTECTION'] = 'false'\n",
    "\n",
    "# Create .streamlit config directory\n",
    "!mkdir -p ~/.streamlit/\n",
    "\n",
    "# Create streamlit config file\n",
    "config_content = \"\"\"\n",
    "[server]\n",
    "headless = true\n",
    "enableXsrfProtection = false\n",
    "port = 8501\n",
    "\n",
    "[client]\n",
    "showErrorDetails = true\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.expanduser('~/.streamlit/config.toml'), 'w') as f:\n",
    "    f.write(config_content)\n",
    "\n",
    "print(\"‚úÖ Streamlit configured for Colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8cbc66",
   "metadata": {},
   "source": [
    "## Section 5: Run Streamlit with Ngrok Tunnel\n",
    "Execute the Streamlit app and establish public URL access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b877ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "from pyngrok import ngrok\n",
    "\n",
    "# Start Streamlit app in background\n",
    "process = subprocess.Popen(\n",
    "    ['streamlit', 'run', 'hybrid_rag_colab.py', '--client.showErrorDetails=false'],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE\n",
    ")\n",
    "\n",
    "# Wait for Streamlit to start\n",
    "print(\"‚è≥ Starting Streamlit app...\")\n",
    "time.sleep(3)\n",
    "\n",
    "# Get the public URL from Ngrok\n",
    "try:\n",
    "    # Kill any existing tunnels\n",
    "    ngrok.disconnect()\n",
    "    \n",
    "    # Create new tunnel\n",
    "    public_url = ngrok.connect(8501, \"tcp\")\n",
    "    print(f\"\\n‚úÖ Streamlit app is running!\")\n",
    "    print(f\"üåê Public URL: {public_url}\")\n",
    "    print(f\"\\nüì± Open this link in your browser to access the app:\")\n",
    "    print(f\"   {public_url}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating tunnel: {e}\")\n",
    "    print(\"Make sure you have set your Ngrok token in the cell above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb6fda6",
   "metadata": {},
   "source": [
    "## Section 6: Access the App via Public URL\n",
    "\n",
    "### Steps to Access:\n",
    "1. **Get Ngrok Token**: Go to https://dashboard.ngrok.com/auth/your-authtoken\n",
    "2. **Copy Your Token**: Add it to the \"Set Up Ngrok Authentication\" cell above\n",
    "3. **Run All Cells**: Execute the cells in order\n",
    "4. **Access the App**: Click the link printed in the \"Run Streamlit with Ngrok Tunnel\" cell\n",
    "5. **Interact**: Use the text input to ask questions to your Hybrid RAG System\n",
    "\n",
    "### Key Modifications for Colab:\n",
    "- ‚úÖ Uses demo corpus instead of full URL scraping (faster)\n",
    "- ‚úÖ Configured Streamlit for headless execution\n",
    "- ‚úÖ Ngrok tunnel provides public access\n",
    "- ‚úÖ Reduced model sizes for faster loading\n",
    "- ‚úÖ Simplified UI for Colab environment"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
