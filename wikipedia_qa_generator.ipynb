{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wikipedia Q&A Pair Generator\n",
        "This notebook fetches content from Wikipedia URLs and generates question-answer pairs in a structured format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (uncomment if needed)\n",
        "# !pip install requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "from typing import List, Dict\n",
        "from pathlib import Path\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WikipediaQAGenerator:\n",
        "    def __init__(self, urls_file: str):\n",
        "        \"\"\"Initialize with URLs file\"\"\"\n",
        "        self.urls_file = Path(urls_file)\n",
        "        self.urls = []\n",
        "        self.articles = []\n",
        "        self.load_urls()\n",
        "        \n",
        "    def load_urls(self):\n",
        "        \"\"\"Load Wikipedia URLs from JSON file\"\"\"\n",
        "        with open(self.urls_file, 'r') as f:\n",
        "            data = json.load(f)\n",
        "            self.urls = data.get('fixed_wiki_urls', [])\n",
        "        print(f\"Loaded {len(self.urls)} Wikipedia URLs\")\n",
        "    \n",
        "    def fetch_wikipedia_content(self, url: str) -> Dict:\n",
        "        \"\"\"Fetch and parse Wikipedia article content\"\"\"\n",
        "        try:\n",
        "            # Add delay to be respectful to Wikipedia servers\n",
        "            time.sleep(0.5)\n",
        "            \n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            \n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            \n",
        "            # Extract title\n",
        "            title_elem = soup.find('h1', {'id': 'firstHeading'})\n",
        "            title = title_elem.text.strip() if title_elem else url.split('/')[-1].replace('_', ' ')\n",
        "            \n",
        "            # Extract main content paragraphs\n",
        "            content_div = soup.find('div', {'id': 'mw-content-text'})\n",
        "            if not content_div:\n",
        "                return None\n",
        "                \n",
        "            paragraphs = content_div.find_all('p')\n",
        "            text_content = []\n",
        "            \n",
        "            for p in paragraphs[:10]:  # Get first 10 paragraphs\n",
        "                text = p.get_text().strip()\n",
        "                if len(text) > 50:  # Filter out very short paragraphs\n",
        "                    text_content.append(text)\n",
        "            \n",
        "            if not text_content:\n",
        "                return None\n",
        "                \n",
        "            return {\n",
        "                'title': title,\n",
        "                'url': url,\n",
        "                'content': ' '.join(text_content[:5]),  # First 5 substantial paragraphs\n",
        "                'source_id': title.replace(' ', '_')\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching {url}: {str(e)}\")\n",
        "            return None\n",
        "    \n",
        "    def fetch_all_articles(self, max_articles: int = None):\n",
        "        \"\"\"Fetch content from all Wikipedia URLs\"\"\"\n",
        "        print(\"Fetching Wikipedia articles...\")\n",
        "        urls_to_fetch = self.urls[:max_articles] if max_articles else self.urls\n",
        "        \n",
        "        for i, url in enumerate(urls_to_fetch, 1):\n",
        "            print(f\"Fetching {i}/{len(urls_to_fetch)}: {url}\")\n",
        "            article = self.fetch_wikipedia_content(url)\n",
        "            if article:\n",
        "                self.articles.append(article)\n",
        "        \n",
        "        print(f\"Successfully fetched {len(self.articles)} articles\")\n",
        "    \n",
        "    def extract_key_facts(self, text: str, title: str) -> List[str]:\n",
        "        \"\"\"Extract key facts from text\"\"\"\n",
        "        facts = []\n",
        "        \n",
        "        # Split into sentences\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "        \n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip()\n",
        "            # Look for informative sentences\n",
        "            if len(sentence) > 30 and any(word in sentence.lower() for word in \n",
        "                                         ['is', 'was', 'are', 'were', 'known', 'called', \n",
        "                                          'located', 'built', 'founded', 'developed', \n",
        "                                          'discovered', 'invented', 'created']):\n",
        "                facts.append(sentence)\n",
        "        \n",
        "        return facts[:5]  # Return up to 5 key facts\n",
        "    \n",
        "    def generate_simple_qa(self, article: Dict) -> Dict:\n",
        "        \"\"\"Generate a simple Q&A pair from an article\"\"\"\n",
        "        title = article['title']\n",
        "        content = article['content']\n",
        "        source_id = article['source_id']\n",
        "        \n",
        "        # Get first meaningful sentence as answer\n",
        "        sentences = [s.strip() for s in re.split(r'[.!?]+', content) if len(s.strip()) > 30]\n",
        "        \n",
        "        if not sentences:\n",
        "            return None\n",
        "        \n",
        "        answer = sentences[0]\n",
        "        \n",
        "        # Determine question based on content patterns\n",
        "        question_templates = [\n",
        "            f'What is {title}?',\n",
        "            f'Tell me about {title}.',\n",
        "            f'What can you tell me about {title}?',\n",
        "            f'Describe {title}.',\n",
        "        ]\n",
        "        \n",
        "        # Try to make more specific questions based on content\n",
        "        if 'located' in answer.lower() or 'in' in answer.lower():\n",
        "            question_templates.insert(0, f'Where is {title} located?')\n",
        "        if 'built' in answer.lower() or 'constructed' in answer.lower():\n",
        "            question_templates.insert(0, f'When was {title} built?')\n",
        "        if 'branch' in answer.lower() or 'field' in answer.lower() or 'studies' in answer.lower():\n",
        "            question_templates.insert(0, f'What does {title} study?')\n",
        "        if 'country' in answer.lower() or 'nation' in answer.lower():\n",
        "            question_templates.insert(0, f'What is {title}?')\n",
        "        \n",
        "        return {\n",
        "            'question': question_templates[0],\n",
        "            'answer': answer,\n",
        "            'source_ids': [source_id],\n",
        "            'category': 'Factual'\n",
        "        }\n",
        "    \n",
        "    def generate_qa_pairs(self, num_pairs: int = 100) -> List[Dict]:\n",
        "        \"\"\"Generate specified number of Q&A pairs\"\"\"\n",
        "        print(f\"\\nGenerating {num_pairs} Q&A pairs...\")\n",
        "        \n",
        "        qa_pairs = []\n",
        "        used_articles = set()\n",
        "        \n",
        "        # Shuffle articles for variety\n",
        "        shuffled_articles = random.sample(self.articles, len(self.articles))\n",
        "        \n",
        "        for article in shuffled_articles:\n",
        "            if len(qa_pairs) >= num_pairs:\n",
        "                break\n",
        "            \n",
        "            # Generate Q&A from this article\n",
        "            qa = self.generate_simple_qa(article)\n",
        "            \n",
        "            if qa and article['source_id'] not in used_articles:\n",
        "                qa_pairs.append(qa)\n",
        "                used_articles.add(article['source_id'])\n",
        "        \n",
        "        # If we need more, generate additional variations\n",
        "        if len(qa_pairs) < num_pairs:\n",
        "            print(f\"Generated {len(qa_pairs)} unique pairs, creating variations to reach {num_pairs}...\")\n",
        "            \n",
        "            while len(qa_pairs) < num_pairs and shuffled_articles:\n",
        "                article = random.choice(shuffled_articles)\n",
        "                qa = self.generate_simple_qa(article)\n",
        "                \n",
        "                if qa:\n",
        "                    # Vary the question slightly\n",
        "                    variations = [\n",
        "                        f'What is {article[\"title\"]}?',\n",
        "                        f'Can you explain what {article[\"title\"]} is?',\n",
        "                        f'Tell me about {article[\"title\"]}.',\n",
        "                        f'Describe {article[\"title\"]}.',\n",
        "                    ]\n",
        "                    qa['question'] = random.choice(variations)\n",
        "                    qa_pairs.append(qa)\n",
        "        \n",
        "        # Limit to requested number\n",
        "        qa_pairs = qa_pairs[:num_pairs]\n",
        "        \n",
        "        # Add IDs\n",
        "        for i, qa in enumerate(qa_pairs, 1):\n",
        "            qa['id'] = i\n",
        "        \n",
        "        print(f\"Successfully generated {len(qa_pairs)} Q&A pairs\")\n",
        "        return qa_pairs\n",
        "    \n",
        "    def save_qa_dataset(self, qa_pairs: List[Dict], output_file: str = None):\n",
        "        \"\"\"Save Q&A pairs in the specified format\"\"\"\n",
        "        if not output_file:\n",
        "            output_file = \"data\\wikipedia_qa_100.json\"\n",
        "        \n",
        "        output_data = {\n",
        "            \"dataset\": \"wikipedia_qa_100\",\n",
        "            \"data\": qa_pairs\n",
        "        }\n",
        "        \n",
        "        output_path = Path(output_file)\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
        "        \n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"✓ Generated {len(qa_pairs)} Q&A pairs\")\n",
        "        print(f\"✓ Saved to {output_path.absolute()}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # Show sample\n",
        "        if qa_pairs:\n",
        "            print(\"\\nSample Q&A pairs:\")\n",
        "            for i, qa in enumerate(qa_pairs[:3], 1):\n",
        "                print(f\"\\n{i}. Question: {qa['question']}\")\n",
        "                print(f\"   Answer: {qa['answer'][:100]}...\")\n",
        "                print(f\"   Source: {qa['source_ids'][0]}\")\n",
        "        \n",
        "        return output_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "Set your input and output file paths here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "urls_file = \"data\\200_fixed_urls.json\"  # Your input file with Wikipedia URLs\n",
        "output_file = \"data\\wikipedia_qa_100.json\"  # Output file for Q&A pairs\n",
        "num_qa_pairs = 100  # Number of Q&A pairs to generate\n",
        "max_articles_to_fetch = 150  # Fetch more than needed for variety"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Q&A Dataset\n",
        "Run this cell to fetch Wikipedia articles and generate Q&A pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"Wikipedia Q&A Dataset Generator\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Initialize generator\n",
        "generator = WikipediaQAGenerator(urls_file)\n",
        "\n",
        "# Fetch Wikipedia articles\n",
        "generator.fetch_all_articles(max_articles=max_articles_to_fetch)\n",
        "\n",
        "if not generator.articles:\n",
        "    print(\"Error: No articles were successfully fetched!\")\n",
        "else:\n",
        "    # Generate Q&A pairs\n",
        "    qa_pairs = generator.generate_qa_pairs(num_pairs=num_qa_pairs)\n",
        "    \n",
        "    # Save to JSON\n",
        "    output_path = generator.save_qa_dataset(qa_pairs, output_file)\n",
        "    \n",
        "    print(f\"\\nDone! Dataset saved to: {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## View Sample Results\n",
        "Display some sample Q&A pairs from the generated dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and display the generated dataset\n",
        "with open(output_file, 'r') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "print(f\"Dataset: {dataset['dataset']}\")\n",
        "print(f\"Total Q&A pairs: {len(dataset['data'])}\\n\")\n",
        "\n",
        "# Show first 5 Q&A pairs\n",
        "for qa in dataset['data'][:5]:\n",
        "    print(f\"ID: {qa['id']}\")\n",
        "    print(f\"Q: {qa['question']}\")\n",
        "    print(f\"A: {qa['answer']}\")\n",
        "    print(f\"Source: {qa['source_ids']}\")\n",
        "    print(f\"Category: {qa['category']}\")\n",
        "    print(\"-\" * 60)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
